{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pdfplumber in c:\\users\\arthur\\anaconda3\\lib\\site-packages (0.11.5)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: pytesseract in c:\\users\\arthur\\anaconda3\\lib\\site-packages (0.3.13)\n",
      "Requirement already satisfied: pdf2image in c:\\users\\arthur\\anaconda3\\lib\\site-packages (1.17.0)\n",
      "Requirement already satisfied: pandas in c:\\users\\arthur\\anaconda3\\lib\\site-packages (1.4.4)\n",
      "Requirement already satisfied: openpyxl in c:\\users\\arthur\\anaconda3\\lib\\site-packages (3.0.10)\n",
      "Requirement already satisfied: Pillow>=9.1 in c:\\users\\arthur\\anaconda3\\lib\\site-packages (from pdfplumber) (9.2.0)\n",
      "Requirement already satisfied: pypdfium2>=4.18.0 in c:\\users\\arthur\\anaconda3\\lib\\site-packages (from pdfplumber) (4.30.1)\n",
      "Requirement already satisfied: pdfminer.six==20231228 in c:\\users\\arthur\\anaconda3\\lib\\site-packages (from pdfplumber) (20231228)\n",
      "Requirement already satisfied: cryptography>=36.0.0 in c:\\users\\arthur\\anaconda3\\lib\\site-packages (from pdfminer.six==20231228->pdfplumber) (37.0.1)\n",
      "Requirement already satisfied: charset-normalizer>=2.0.0 in c:\\users\\arthur\\anaconda3\\lib\\site-packages (from pdfminer.six==20231228->pdfplumber) (2.0.4)\n",
      "Requirement already satisfied: packaging>=21.3 in c:\\users\\arthur\\anaconda3\\lib\\site-packages (from pytesseract) (23.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\arthur\\anaconda3\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: numpy>=1.18.5 in c:\\users\\arthur\\anaconda3\\lib\\site-packages (from pandas) (1.21.5)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\arthur\\anaconda3\\lib\\site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: et_xmlfile in c:\\users\\arthur\\anaconda3\\lib\\site-packages (from openpyxl) (1.1.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\arthur\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
      "Requirement already satisfied: cffi>=1.12 in c:\\users\\arthur\\anaconda3\\lib\\site-packages (from cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (1.15.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\arthur\\anaconda3\\lib\\site-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (2.21)\n"
     ]
    }
   ],
   "source": [
    "pip install pdfplumber pytesseract pdf2image pandas openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytesseract\n",
    "pytesseract.pytesseract.tesseract_cmd = r'C:\\Program Files\\Tesseract-OCR\\tesseract.exe'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "poppler_path = r\"C:\\Users\\Arthur\\Documents\\Python Packages\\poppler-24.08.0\\Library\\bin\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "'return' outside function (1009751755.py, line 70)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\Arthur\\AppData\\Local\\Temp\\ipykernel_4516\\1009751755.py\"\u001b[1;36m, line \u001b[1;32m70\u001b[0m\n\u001b[1;33m    return {\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m 'return' outside function\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pdfplumber\n",
    "import pytesseract\n",
    "from pdf2image import convert_from_path\n",
    "import pandas as pd\n",
    "import re\n",
    "from math import sqrt\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "# üîπ Carregar dicion√°rios para o New Dale-Chall Score\n",
    "\n",
    "with open(\"F:\\\\BACKUP GERAL 01.06.2020\\\\UFSC\\\\2025\\\\Editais\\\\Concurso COLLABCOOP\\\\DICION√ÅRIO\\\\dicionario_palavras_dificeis.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    palavras_dificeis = set(f.read().splitlines())\n",
    "    \n",
    "    \n",
    "    # üîπ Remover t√≠tulos, cap√≠tulos, se√ß√µes, subse√ß√µes e artigos antes de contar senten√ßas\n",
    "    patterns_to_remove = [\n",
    "        r'^\\s*T√çTULO\\s+[IVXLCDM\\d]+',  # T√çTULO I, T√çTULO II, etc.\n",
    "        r'^\\s*CAP√çTULO\\s+[IVXLCDM\\d]+',  # CAP√çTULO I, CAP√çTULO II, etc.\n",
    "        r'^\\s*SE√á√ÉO\\s+[IVXLCDM\\d]+',  # SE√á√ÉO I, SE√á√ÉO II, etc.\n",
    "        r'^\\s*SUBSE√á√ÉO\\s+[IVXLCDM\\d]+',  # SUBSE√á√ÉO I, SUBSE√á√ÉO II, etc.\n",
    "        r'^\\s*Art\\.?\\s*\\d+[¬∫¬∞]?',  # Art. 1¬∫, Art. 2, etc.\n",
    "        r'^\\s*¬ß\\s*\\d+[¬∫¬∞]?',  # ¬ß 1¬∫, ¬ß 2¬∫, etc.\n",
    "        r'^\\s*Par√°grafo √∫nico\\.',  # \"Par√°grafo √∫nico.\"\n",
    "    ]\n",
    "\n",
    "    # Aplicar a remo√ß√£o das senten√ßas irrelevantes\n",
    "    sentences = [\n",
    "        sentence.strip() for sentence in sentences\n",
    "        if sentence.strip() and not any(re.match(pattern, sentence.strip(), re.IGNORECASE) for pattern in patterns_to_remove)\n",
    "    ]\n",
    "\n",
    "    # Atualizar a contagem de senten√ßas ap√≥s a filtragem\n",
    "    S = max(len(sentences), 1)\n",
    "\n",
    "    # C√°lculo das palavras longas e complexas\n",
    "    long_words = [w for w in words if len(w) > 6]\n",
    "    complex_words = [w for w in words if len(re.findall(r'[aeiou√°√©√≠√≥√∫√¢√™√Æ√¥√ª√£√µ√†]{2,}', w, re.I)) > 1]\n",
    "    syllables = sum(len(re.findall(r'[aeiou√°√©√≠√≥√∫√¢√™√Æ√¥√ª√£√µ√†]{1,2}', w, re.I)) for w in words)\n",
    "\n",
    "    W = len(words)\n",
    "    LW = len(long_words)\n",
    "    CW = len(complex_words)\n",
    "    C = sum(len(word) for word in words if word.isalpha())\n",
    "\n",
    "    # üîπ Contagem de par√°grafos corrigida\n",
    "    paragraph_count = len(re.findall(r'\\n\\s*\\n', text))\n",
    "    if paragraph_count == 0:\n",
    "        paragraph_count = max(S // 3, 1)\n",
    "\n",
    "    # üîπ √çndices de legibilidade\n",
    "    flesch = round(0.39 * (W / S) + 11.8 * (syllables / W) - 15.59, 2) if W > 0 and S > 0 else 0\n",
    "    ari = round(4.71 * (C / W) + 0.5 * (W / S) - 21.43, 2) if W > 0 and S > 0 else 0\n",
    "    lix = round((100 * LW / W) + (W / S), 2) if W > 0 else 0\n",
    "    rix = round(LW / S, 2) if S > 0 else 0\n",
    "    fog = round(0.4 * ((W / S) + (40 * CW / W)), 2) if W > 0 and S > 0 else 0\n",
    "    smog = round(1.043 * sqrt(30 * (CW / S)) + 3.1291, 2) if S > 0 and CW > 0 else 0\n",
    "\n",
    "    # üîπ New Dale-Chall Score\n",
    "    difficult_words = sum(1 for w in words if w.lower() in palavras_dificeis)\n",
    "    ndc = round(0.1579 * (difficult_words / W * 100) + 0.0496 * (W / S) + 3.6365, 2) if W > 0 and S > 0 else 0\n",
    "\n",
    "    # üîπ Type-Token Ratio (TTR)\n",
    "    ttr = round(len(set(words)) / W, 4) if W > 0 else 0\n",
    "\n",
    "    # üîπ Herdan‚Äôs C (Complexidade Lexical)\n",
    "    herdan_c = round(sqrt(len(set(words)) / W), 4) if W > 0 else 0\n",
    "\n",
    "    return {\n",
    "        'LIX Index': lix,\n",
    "        'RIX Index': rix,\n",
    "        'ARI Index': ari,\n",
    "        'Flesch-Kincaid': flesch,\n",
    "        'Fog Index': fog,\n",
    "        'SMOG Index': smog,\n",
    "        'New Dale-Chall': ndc,\n",
    "        'Type-Token Ratio (TTR)': ttr,\n",
    "        'Herdan‚Äôs C': herdan_c,\n",
    "        'Paragraph Count': paragraph_count\n",
    "    }\n",
    "\n",
    "# üîπ Fun√ß√£o para contar T√çTULO, CAP√çTULO, SE√á√ÉO e ARTIGOS\n",
    "def count_sections(text):\n",
    "    title_count = len(re.findall(r'^\\s*T√çTULO\\s+[IVXLCDM\\d]+\\s*$', text, re.MULTILINE | re.IGNORECASE))\n",
    "    chapter_count = len(re.findall(r'^\\s*CAP√çTULO\\s+[IVXLCDM\\d]+\\s*$', text, re.MULTILINE | re.IGNORECASE))\n",
    "    section_count = len(re.findall(r'^\\s*SE√á√ÉO\\s+[IVXLCDM\\d]+\\s*$', text, re.MULTILINE | re.IGNORECASE))\n",
    "    article_count = len(re.findall(r'^\\s*Art\\.?\\s*\\d+', text, re.MULTILINE | re.IGNORECASE))\n",
    "\n",
    "    return {\n",
    "        'Title Count': title_count,\n",
    "        'Chapter Count': chapter_count,\n",
    "        'Section Count': section_count,\n",
    "        'Article Count': article_count\n",
    "    }\n",
    "\n",
    "# üîπ Fun√ß√£o para extrair CNPJ\n",
    "def extract_cnpj(text):\n",
    "    match = re.search(r'CNPJ\\D*(\\d{2}\\.?\\d{3}\\.?\\d{3}/?\\d{4}-?\\d{2})', text)\n",
    "    return match.group(1).replace(\".\", \"\").replace(\"/\", \"\").replace(\"-\", \"\") if match else None\n",
    "\n",
    "# üîπ Fun√ß√£o para calcular estat√≠sticas do texto\n",
    "def get_text_stats(text):\n",
    "    words = text.split()\n",
    "    sentences = re.split(r'(?<=[.!?])\\s+|\\nArt\\.\\s*\\d+', text)\n",
    "    character_count = sum(len(word) for word in words)\n",
    "    unique_words = len(set(words))\n",
    "    readability_indices = calculate_readability_indices(text)\n",
    "    section_counts = count_sections(text)\n",
    "\n",
    "    return {\n",
    "        'Word Count': len(words),\n",
    "        'Sentence Count': len(sentences),\n",
    "        'Character Count': character_count,\n",
    "        'Unique Words': unique_words,\n",
    "        **readability_indices,\n",
    "        **section_counts\n",
    "    }\n",
    "\n",
    "# üîπ Escolha do Ano\n",
    "year = 2016  # Altere conforme necess√°rio\n",
    "\n",
    "# üîπ Diret√≥rios baseados no ano\n",
    "directory_path = fr\"F:\\BACKUP GERAL 01.06.2020\\UFSC\\2024\\Artigo 2 - TESE\\Estatutos\\{year}\"\n",
    "output_directory = fr\"F:\\BACKUP GERAL 01.06.2020\\UFSC\\2025\\Editais\\Concurso COLLABCOOP\\Output final\"\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "output_file = os.path.join(output_directory, f\"estatutos_resultados_{year}.xlsx\")\n",
    "\n",
    "# üîπ Processar os PDFs\n",
    "data = []\n",
    "for filename in os.listdir(directory_path):\n",
    "    file_path = os.path.join(directory_path, filename)\n",
    "    if filename.endswith(\".pdf\") and os.path.isfile(file_path):\n",
    "        text = extract_text_from_pdf(file_path)\n",
    "        if not text:\n",
    "            continue\n",
    "        cnpj = extract_cnpj(text)\n",
    "        stats = get_text_stats(text)\n",
    "        stats['File Name'] = filename\n",
    "        stats['CNPJ'] = cnpj\n",
    "        stats['Page Count'] = len(pdfplumber.open(file_path).pages)\n",
    "        stats['File Size (KB)'] = round(Path(file_path).stat().st_size / 1024, 2)\n",
    "        data.append(stats)\n",
    "\n",
    "result_df = pd.DataFrame(data)\n",
    "\n",
    "# üîπ Organizar colunas com \"File Name\" e \"CNPJ\" na frente\n",
    "column_order = ['File Name', 'CNPJ'] + [col for col in result_df.columns if col not in ['File Name', 'CNPJ']]\n",
    "result_df = result_df[column_order]\n",
    "\n",
    "# üîπ Salvar no Excel\n",
    "result_df.to_excel(output_file, index=False, engine=\"openpyxl\")\n",
    "\n",
    "print(f\"‚úÖ Processamento conclu√≠do! Dados salvos em: {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EXTRA√á√ÉO - 2016"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Processamento conclu√≠do! Dados salvos em: F:\\BACKUP GERAL 01.06.2020\\UFSC\\2025\\Editais\\Concurso COLLABCOOP\\Output final\\estatutos_resultados_2016.xlsx\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pdfplumber\n",
    "import pytesseract\n",
    "from pdf2image import convert_from_path\n",
    "import pandas as pd\n",
    "import re\n",
    "from math import sqrt\n",
    "from pathlib import Path\n",
    "\n",
    "# üîπ Carregar dicion√°rio de palavras dif√≠ceis para New Dale-Chall Score\n",
    "with open(\"F:\\\\BACKUP GERAL 01.06.2020\\\\UFSC\\\\2025\\\\Editais\\\\Concurso COLLABCOOP\\\\DICION√ÅRIO\\\\dicionario_palavras_dificeis.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    palavras_dificeis = set(f.read().splitlines())\n",
    "\n",
    "# üîπ Fun√ß√£o para calcular √≠ndices de legibilidade\n",
    "def calculate_readability_indices(text):\n",
    "    patterns_to_remove = [\n",
    "        r'^\\s*T√çTULO\\s+[IVXLCDM\\d]+',\n",
    "        r'^\\s*CAP√çTULO\\s+[IVXLCDM\\d]+',\n",
    "        r'^\\s*SE√á√ÉO\\s+[IVXLCDM\\d]+',\n",
    "        r'^\\s*SUBSE√á√ÉO\\s+[IVXLCDM\\d]+',\n",
    "        r'^\\s*Art\\.?\\s*\\d+[¬∫¬∞]?',\n",
    "        r'^\\s*¬ß\\s*\\d+[¬∫¬∞]?',\n",
    "        r'^\\s*Par√°grafo √∫nico\\.',\n",
    "    ]\n",
    "\n",
    "    sentences = re.split(r'(?<=[.!?])\\s+|\\nArt\\.\\s*\\d+', text)\n",
    "    sentences = [\n",
    "        sentence.strip() for sentence in sentences\n",
    "        if sentence.strip() and not any(re.match(pattern, sentence.strip(), re.IGNORECASE) for pattern in patterns_to_remove)\n",
    "    ]\n",
    "    S = max(len(sentences), 1)\n",
    "\n",
    "    words = text.split()\n",
    "    long_words = [w for w in words if len(w) > 6]\n",
    "    complex_words = [w for w in words if len(re.findall(r'[aeiou√°√©√≠√≥√∫√¢√™√Æ√¥√ª√£√µ√†]{2,}', w, re.I)) > 1]\n",
    "    syllables = sum(len(re.findall(r'[aeiou√°√©√≠√≥√∫√¢√™√Æ√¥√ª√£√µ√†]{1,2}', w, re.I)) for w in words)\n",
    "\n",
    "    W = len(words)\n",
    "    LW = len(long_words)\n",
    "    CW = len(complex_words)\n",
    "    C = sum(len(word) for word in words if word.isalpha())\n",
    "\n",
    "    paragraph_count = len(re.findall(r'\\n\\s*\\n', text))\n",
    "    if paragraph_count == 0:\n",
    "        paragraph_count = max(S // 3, 1)\n",
    "\n",
    "    flesch = round(0.39 * (W / S) + 11.8 * (syllables / W) - 15.59, 2) if W > 0 and S > 0 else 0\n",
    "    ari = round(4.71 * (C / W) + 0.5 * (W / S) - 21.43, 2) if W > 0 and S > 0 else 0\n",
    "    lix = round((100 * LW / W) + (W / S), 2) if W > 0 else 0\n",
    "    rix = round(LW / S, 2) if S > 0 else 0\n",
    "    fog = round(0.4 * ((W / S) + (40 * CW / W)), 2) if W > 0 and S > 0 else 0\n",
    "    smog = round(1.043 * sqrt(30 * (CW / S)) + 3.1291, 2) if S > 0 and CW > 0 else 0\n",
    "\n",
    "    difficult_words = sum(1 for w in words if w.lower() in palavras_dificeis)\n",
    "    ndc = round(0.1579 * (difficult_words / W * 100) + 0.0496 * (W / S) + 3.6365, 2) if W > 0 and S > 0 else 0\n",
    "\n",
    "    ttr = round(len(set(words)) / W, 4) if W > 0 else 0\n",
    "    herdan_c = round(sqrt(len(set(words)) / W), 4) if W > 0 else 0\n",
    "\n",
    "    return {\n",
    "        'LIX Index': lix,\n",
    "        'RIX Index': rix,\n",
    "        'ARI Index': ari,\n",
    "        'Flesch-Kincaid': flesch,\n",
    "        'Fog Index': fog,\n",
    "        'SMOG Index': smog,\n",
    "        'New Dale-Chall': ndc,\n",
    "        'Type-Token Ratio (TTR)': ttr,\n",
    "        'Herdan‚Äôs C': herdan_c,\n",
    "        'Paragraph Count': paragraph_count\n",
    "    }\n",
    "\n",
    "# üîπ Fun√ß√£o para contar se√ß√µes, cap√≠tulos e artigos\n",
    "def count_sections(text):\n",
    "    title_count = len(re.findall(r'^\\s*T√çTULO\\s+[IVXLCDM\\d]+\\s*$', text, re.MULTILINE | re.IGNORECASE))\n",
    "    chapter_count = len(re.findall(r'^\\s*CAP√çTULO\\s+[IVXLCDM\\d]+\\s*$', text, re.MULTILINE | re.IGNORECASE))\n",
    "    section_count = len(re.findall(r'^\\s*SE√á√ÉO\\s+[IVXLCDM\\d]+\\s*$', text, re.MULTILINE | re.IGNORECASE))\n",
    "    article_count = len(re.findall(r'^\\s*Art\\.?\\s*\\d+', text, re.MULTILINE | re.IGNORECASE))\n",
    "\n",
    "    return {\n",
    "        'Title Count': title_count,\n",
    "        'Chapter Count': chapter_count,\n",
    "        'Section Count': section_count,\n",
    "        'Article Count': article_count\n",
    "    }\n",
    "\n",
    "# üîπ Fun√ß√£o para extrair CNPJ\n",
    "def extract_cnpj(text):\n",
    "    match = re.search(r'CNPJ\\D*(\\d{2}\\.?\\d{3}\\.?\\d{3}/?\\d{4}-?\\d{2})', text)\n",
    "    return match.group(1).replace(\".\", \"\").replace(\"/\", \"\").replace(\"-\", \"\") if match else None\n",
    "\n",
    "# üîπ Fun√ß√£o para calcular estat√≠sticas do texto\n",
    "def get_text_stats(text):\n",
    "    words = text.split()\n",
    "    sentences = re.split(r'(?<=[.!?])\\s+|\\nArt\\.\\s*\\d+', text)\n",
    "    character_count = sum(len(word) for word in words)\n",
    "    unique_words = len(set(words))\n",
    "    readability_indices = calculate_readability_indices(text)\n",
    "    section_counts = count_sections(text)\n",
    "\n",
    "    return {\n",
    "        'Word Count': len(words),\n",
    "        'Sentence Count': len(sentences),\n",
    "        'Character Count': character_count,\n",
    "        'Unique Words': unique_words,\n",
    "        **readability_indices,\n",
    "        **section_counts\n",
    "    }\n",
    "\n",
    "# üîπ Processar os PDFs\n",
    "year = 2016\n",
    "directory_path = fr\"F:\\BACKUP GERAL 01.06.2020\\UFSC\\2024\\Artigo 2 - TESE\\Estatutos\\{year}\"\n",
    "output_directory = fr\"F:\\BACKUP GERAL 01.06.2020\\UFSC\\2025\\Editais\\Concurso COLLABCOOP\\Output final\"\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "output_file = os.path.join(output_directory, f\"estatutos_resultados_{year}.xlsx\")\n",
    "\n",
    "data = []\n",
    "for filename in os.listdir(directory_path):\n",
    "    file_path = os.path.join(directory_path, filename)\n",
    "    if filename.endswith(\".pdf\") and os.path.isfile(file_path):\n",
    "        text = extract_text_from_pdf(file_path)\n",
    "        if not text:\n",
    "            continue\n",
    "        cnpj = extract_cnpj(text)\n",
    "        stats = get_text_stats(text)\n",
    "        stats['File Name'] = filename\n",
    "        stats['CNPJ'] = cnpj\n",
    "        stats['Page Count'] = len(pdfplumber.open(file_path).pages)\n",
    "        stats['File Size (KB)'] = round(Path(file_path).stat().st_size / 1024, 2)\n",
    "        data.append(stats)\n",
    "\n",
    "result_df = pd.DataFrame(data)\n",
    "column_order = ['File Name', 'CNPJ'] + [col for col in result_df.columns if col not in ['File Name', 'CNPJ']]\n",
    "result_df = result_df[column_order]\n",
    "\n",
    "# üîπ Salvar no Excel\n",
    "result_df.to_excel(output_file, index=False, engine=\"openpyxl\")\n",
    "print(f\"‚úÖ Processamento conclu√≠do! Dados salvos em: {output_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EXTRA√á√ÉO - 2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Processamento conclu√≠do! Dados salvos em: F:\\BACKUP GERAL 01.06.2020\\UFSC\\2025\\Editais\\Concurso COLLABCOOP\\Output final\\estatutos_resultados_2017.xlsx\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pdfplumber\n",
    "import pytesseract\n",
    "from pdf2image import convert_from_path\n",
    "import pandas as pd\n",
    "import re\n",
    "from math import sqrt\n",
    "from pathlib import Path\n",
    "\n",
    "# üîπ Carregar dicion√°rio de palavras dif√≠ceis para New Dale-Chall Score\n",
    "with open(\"F:\\\\BACKUP GERAL 01.06.2020\\\\UFSC\\\\2025\\\\Editais\\\\Concurso COLLABCOOP\\\\DICION√ÅRIO\\\\dicionario_palavras_dificeis.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    palavras_dificeis = set(f.read().splitlines())\n",
    "\n",
    "# üîπ Fun√ß√£o para calcular √≠ndices de legibilidade\n",
    "def calculate_readability_indices(text):\n",
    "    patterns_to_remove = [\n",
    "        r'^\\s*T√çTULO\\s+[IVXLCDM\\d]+',\n",
    "        r'^\\s*CAP√çTULO\\s+[IVXLCDM\\d]+',\n",
    "        r'^\\s*SE√á√ÉO\\s+[IVXLCDM\\d]+',\n",
    "        r'^\\s*SUBSE√á√ÉO\\s+[IVXLCDM\\d]+',\n",
    "        r'^\\s*Art\\.?\\s*\\d+[¬∫¬∞]?',\n",
    "        r'^\\s*¬ß\\s*\\d+[¬∫¬∞]?',\n",
    "        r'^\\s*Par√°grafo √∫nico\\.',\n",
    "    ]\n",
    "\n",
    "    sentences = re.split(r'(?<=[.!?])\\s+|\\nArt\\.\\s*\\d+', text)\n",
    "    sentences = [\n",
    "        sentence.strip() for sentence in sentences\n",
    "        if sentence.strip() and not any(re.match(pattern, sentence.strip(), re.IGNORECASE) for pattern in patterns_to_remove)\n",
    "    ]\n",
    "    S = max(len(sentences), 1)\n",
    "\n",
    "    words = text.split()\n",
    "    long_words = [w for w in words if len(w) > 6]\n",
    "    complex_words = [w for w in words if len(re.findall(r'[aeiou√°√©√≠√≥√∫√¢√™√Æ√¥√ª√£√µ√†]{2,}', w, re.I)) > 1]\n",
    "    syllables = sum(len(re.findall(r'[aeiou√°√©√≠√≥√∫√¢√™√Æ√¥√ª√£√µ√†]{1,2}', w, re.I)) for w in words)\n",
    "\n",
    "    W = len(words)\n",
    "    LW = len(long_words)\n",
    "    CW = len(complex_words)\n",
    "    C = sum(len(word) for word in words if word.isalpha())\n",
    "\n",
    "    paragraph_count = len(re.findall(r'\\n\\s*\\n', text))\n",
    "    if paragraph_count == 0:\n",
    "        paragraph_count = max(S // 3, 1)\n",
    "\n",
    "    flesch = round(0.39 * (W / S) + 11.8 * (syllables / W) - 15.59, 2) if W > 0 and S > 0 else 0\n",
    "    ari = round(4.71 * (C / W) + 0.5 * (W / S) - 21.43, 2) if W > 0 and S > 0 else 0\n",
    "    lix = round((100 * LW / W) + (W / S), 2) if W > 0 else 0\n",
    "    rix = round(LW / S, 2) if S > 0 else 0\n",
    "    fog = round(0.4 * ((W / S) + (40 * CW / W)), 2) if W > 0 and S > 0 else 0\n",
    "    smog = round(1.043 * sqrt(30 * (CW / S)) + 3.1291, 2) if S > 0 and CW > 0 else 0\n",
    "\n",
    "    difficult_words = sum(1 for w in words if w.lower() in palavras_dificeis)\n",
    "    ndc = round(0.1579 * (difficult_words / W * 100) + 0.0496 * (W / S) + 3.6365, 2) if W > 0 and S > 0 else 0\n",
    "\n",
    "    ttr = round(len(set(words)) / W, 4) if W > 0 else 0\n",
    "    herdan_c = round(sqrt(len(set(words)) / W), 4) if W > 0 else 0\n",
    "\n",
    "    return {\n",
    "        'LIX Index': lix,\n",
    "        'RIX Index': rix,\n",
    "        'ARI Index': ari,\n",
    "        'Flesch-Kincaid': flesch,\n",
    "        'Fog Index': fog,\n",
    "        'SMOG Index': smog,\n",
    "        'New Dale-Chall': ndc,\n",
    "        'Type-Token Ratio (TTR)': ttr,\n",
    "        'Herdan‚Äôs C': herdan_c,\n",
    "        'Paragraph Count': paragraph_count\n",
    "    }\n",
    "\n",
    "# üîπ Fun√ß√£o para contar se√ß√µes, cap√≠tulos e artigos\n",
    "def count_sections(text):\n",
    "    title_count = len(re.findall(r'^\\s*T√çTULO\\s+[IVXLCDM\\d]+\\s*$', text, re.MULTILINE | re.IGNORECASE))\n",
    "    chapter_count = len(re.findall(r'^\\s*CAP√çTULO\\s+[IVXLCDM\\d]+\\s*$', text, re.MULTILINE | re.IGNORECASE))\n",
    "    section_count = len(re.findall(r'^\\s*SE√á√ÉO\\s+[IVXLCDM\\d]+\\s*$', text, re.MULTILINE | re.IGNORECASE))\n",
    "    article_count = len(re.findall(r'^\\s*Art\\.?\\s*\\d+', text, re.MULTILINE | re.IGNORECASE))\n",
    "\n",
    "    return {\n",
    "        'Title Count': title_count,\n",
    "        'Chapter Count': chapter_count,\n",
    "        'Section Count': section_count,\n",
    "        'Article Count': article_count\n",
    "    }\n",
    "\n",
    "# üîπ Fun√ß√£o para extrair CNPJ\n",
    "def extract_cnpj(text):\n",
    "    match = re.search(r'CNPJ\\D*(\\d{2}\\.?\\d{3}\\.?\\d{3}/?\\d{4}-?\\d{2})', text)\n",
    "    return match.group(1).replace(\".\", \"\").replace(\"/\", \"\").replace(\"-\", \"\") if match else None\n",
    "\n",
    "# üîπ Fun√ß√£o para calcular estat√≠sticas do texto\n",
    "def get_text_stats(text):\n",
    "    words = text.split()\n",
    "    sentences = re.split(r'(?<=[.!?])\\s+|\\nArt\\.\\s*\\d+', text)\n",
    "    character_count = sum(len(word) for word in words)\n",
    "    unique_words = len(set(words))\n",
    "    readability_indices = calculate_readability_indices(text)\n",
    "    section_counts = count_sections(text)\n",
    "\n",
    "    return {\n",
    "        'Word Count': len(words),\n",
    "        'Sentence Count': len(sentences),\n",
    "        'Character Count': character_count,\n",
    "        'Unique Words': unique_words,\n",
    "        **readability_indices,\n",
    "        **section_counts\n",
    "    }\n",
    "\n",
    "# üîπ Processar os PDFs\n",
    "year = 2017\n",
    "directory_path = fr\"F:\\BACKUP GERAL 01.06.2020\\UFSC\\2024\\Artigo 2 - TESE\\Estatutos\\{year}\"\n",
    "output_directory = fr\"F:\\BACKUP GERAL 01.06.2020\\UFSC\\2025\\Editais\\Concurso COLLABCOOP\\Output final\"\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "output_file = os.path.join(output_directory, f\"estatutos_resultados_{year}.xlsx\")\n",
    "\n",
    "data = []\n",
    "for filename in os.listdir(directory_path):\n",
    "    file_path = os.path.join(directory_path, filename)\n",
    "    if filename.endswith(\".pdf\") and os.path.isfile(file_path):\n",
    "        text = extract_text_from_pdf(file_path)\n",
    "        if not text:\n",
    "            continue\n",
    "        cnpj = extract_cnpj(text)\n",
    "        stats = get_text_stats(text)\n",
    "        stats['File Name'] = filename\n",
    "        stats['CNPJ'] = cnpj\n",
    "        stats['Page Count'] = len(pdfplumber.open(file_path).pages)\n",
    "        stats['File Size (KB)'] = round(Path(file_path).stat().st_size / 1024, 2)\n",
    "        data.append(stats)\n",
    "\n",
    "result_df = pd.DataFrame(data)\n",
    "column_order = ['File Name', 'CNPJ'] + [col for col in result_df.columns if col not in ['File Name', 'CNPJ']]\n",
    "result_df = result_df[column_order]\n",
    "\n",
    "# üîπ Salvar no Excel\n",
    "result_df.to_excel(output_file, index=False, engine=\"openpyxl\")\n",
    "print(f\"‚úÖ Processamento conclu√≠do! Dados salvos em: {output_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EXTRA√á√ÉO - 2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Processamento conclu√≠do! Dados salvos em: F:\\BACKUP GERAL 01.06.2020\\UFSC\\2025\\Editais\\Concurso COLLABCOOP\\Output final\\estatutos_resultados_2018.xlsx\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pdfplumber\n",
    "import pytesseract\n",
    "from pdf2image import convert_from_path\n",
    "import pandas as pd\n",
    "import re\n",
    "from math import sqrt\n",
    "from pathlib import Path\n",
    "\n",
    "# üîπ Carregar dicion√°rio de palavras dif√≠ceis para New Dale-Chall Score\n",
    "with open(\"F:\\\\BACKUP GERAL 01.06.2020\\\\UFSC\\\\2025\\\\Editais\\\\Concurso COLLABCOOP\\\\DICION√ÅRIO\\\\dicionario_palavras_dificeis.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    palavras_dificeis = set(f.read().splitlines())\n",
    "\n",
    "# üîπ Fun√ß√£o para calcular √≠ndices de legibilidade\n",
    "def calculate_readability_indices(text):\n",
    "    patterns_to_remove = [\n",
    "        r'^\\s*T√çTULO\\s+[IVXLCDM\\d]+',\n",
    "        r'^\\s*CAP√çTULO\\s+[IVXLCDM\\d]+',\n",
    "        r'^\\s*SE√á√ÉO\\s+[IVXLCDM\\d]+',\n",
    "        r'^\\s*SUBSE√á√ÉO\\s+[IVXLCDM\\d]+',\n",
    "        r'^\\s*Art\\.?\\s*\\d+[¬∫¬∞]?',\n",
    "        r'^\\s*¬ß\\s*\\d+[¬∫¬∞]?',\n",
    "        r'^\\s*Par√°grafo √∫nico\\.',\n",
    "    ]\n",
    "\n",
    "    sentences = re.split(r'(?<=[.!?])\\s+|\\nArt\\.\\s*\\d+', text)\n",
    "    sentences = [\n",
    "        sentence.strip() for sentence in sentences\n",
    "        if sentence.strip() and not any(re.match(pattern, sentence.strip(), re.IGNORECASE) for pattern in patterns_to_remove)\n",
    "    ]\n",
    "    S = max(len(sentences), 1)\n",
    "\n",
    "    words = text.split()\n",
    "    long_words = [w for w in words if len(w) > 6]\n",
    "    complex_words = [w for w in words if len(re.findall(r'[aeiou√°√©√≠√≥√∫√¢√™√Æ√¥√ª√£√µ√†]{2,}', w, re.I)) > 1]\n",
    "    syllables = sum(len(re.findall(r'[aeiou√°√©√≠√≥√∫√¢√™√Æ√¥√ª√£√µ√†]{1,2}', w, re.I)) for w in words)\n",
    "\n",
    "    W = len(words)\n",
    "    LW = len(long_words)\n",
    "    CW = len(complex_words)\n",
    "    C = sum(len(word) for word in words if word.isalpha())\n",
    "\n",
    "    paragraph_count = len(re.findall(r'\\n\\s*\\n', text))\n",
    "    if paragraph_count == 0:\n",
    "        paragraph_count = max(S // 3, 1)\n",
    "\n",
    "    flesch = round(0.39 * (W / S) + 11.8 * (syllables / W) - 15.59, 2) if W > 0 and S > 0 else 0\n",
    "    ari = round(4.71 * (C / W) + 0.5 * (W / S) - 21.43, 2) if W > 0 and S > 0 else 0\n",
    "    lix = round((100 * LW / W) + (W / S), 2) if W > 0 else 0\n",
    "    rix = round(LW / S, 2) if S > 0 else 0\n",
    "    fog = round(0.4 * ((W / S) + (40 * CW / W)), 2) if W > 0 and S > 0 else 0\n",
    "    smog = round(1.043 * sqrt(30 * (CW / S)) + 3.1291, 2) if S > 0 and CW > 0 else 0\n",
    "\n",
    "    difficult_words = sum(1 for w in words if w.lower() in palavras_dificeis)\n",
    "    ndc = round(0.1579 * (difficult_words / W * 100) + 0.0496 * (W / S) + 3.6365, 2) if W > 0 and S > 0 else 0\n",
    "\n",
    "    ttr = round(len(set(words)) / W, 4) if W > 0 else 0\n",
    "    herdan_c = round(sqrt(len(set(words)) / W), 4) if W > 0 else 0\n",
    "\n",
    "    return {\n",
    "        'LIX Index': lix,\n",
    "        'RIX Index': rix,\n",
    "        'ARI Index': ari,\n",
    "        'Flesch-Kincaid': flesch,\n",
    "        'Fog Index': fog,\n",
    "        'SMOG Index': smog,\n",
    "        'New Dale-Chall': ndc,\n",
    "        'Type-Token Ratio (TTR)': ttr,\n",
    "        'Herdan‚Äôs C': herdan_c,\n",
    "        'Paragraph Count': paragraph_count\n",
    "    }\n",
    "\n",
    "# üîπ Fun√ß√£o para contar se√ß√µes, cap√≠tulos e artigos\n",
    "def count_sections(text):\n",
    "    title_count = len(re.findall(r'^\\s*T√çTULO\\s+[IVXLCDM\\d]+\\s*$', text, re.MULTILINE | re.IGNORECASE))\n",
    "    chapter_count = len(re.findall(r'^\\s*CAP√çTULO\\s+[IVXLCDM\\d]+\\s*$', text, re.MULTILINE | re.IGNORECASE))\n",
    "    section_count = len(re.findall(r'^\\s*SE√á√ÉO\\s+[IVXLCDM\\d]+\\s*$', text, re.MULTILINE | re.IGNORECASE))\n",
    "    article_count = len(re.findall(r'^\\s*Art\\.?\\s*\\d+', text, re.MULTILINE | re.IGNORECASE))\n",
    "\n",
    "    return {\n",
    "        'Title Count': title_count,\n",
    "        'Chapter Count': chapter_count,\n",
    "        'Section Count': section_count,\n",
    "        'Article Count': article_count\n",
    "    }\n",
    "\n",
    "# üîπ Fun√ß√£o para extrair CNPJ\n",
    "def extract_cnpj(text):\n",
    "    match = re.search(r'CNPJ\\D*(\\d{2}\\.?\\d{3}\\.?\\d{3}/?\\d{4}-?\\d{2})', text)\n",
    "    return match.group(1).replace(\".\", \"\").replace(\"/\", \"\").replace(\"-\", \"\") if match else None\n",
    "\n",
    "# üîπ Fun√ß√£o para calcular estat√≠sticas do texto\n",
    "def get_text_stats(text):\n",
    "    words = text.split()\n",
    "    sentences = re.split(r'(?<=[.!?])\\s+|\\nArt\\.\\s*\\d+', text)\n",
    "    character_count = sum(len(word) for word in words)\n",
    "    unique_words = len(set(words))\n",
    "    readability_indices = calculate_readability_indices(text)\n",
    "    section_counts = count_sections(text)\n",
    "\n",
    "    return {\n",
    "        'Word Count': len(words),\n",
    "        'Sentence Count': len(sentences),\n",
    "        'Character Count': character_count,\n",
    "        'Unique Words': unique_words,\n",
    "        **readability_indices,\n",
    "        **section_counts\n",
    "    }\n",
    "\n",
    "# üîπ Processar os PDFs\n",
    "year = 2018\n",
    "directory_path = fr\"F:\\BACKUP GERAL 01.06.2020\\UFSC\\2024\\Artigo 2 - TESE\\Estatutos\\{year}\"\n",
    "output_directory = fr\"F:\\BACKUP GERAL 01.06.2020\\UFSC\\2025\\Editais\\Concurso COLLABCOOP\\Output final\"\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "output_file = os.path.join(output_directory, f\"estatutos_resultados_{year}.xlsx\")\n",
    "\n",
    "data = []\n",
    "for filename in os.listdir(directory_path):\n",
    "    file_path = os.path.join(directory_path, filename)\n",
    "    if filename.endswith(\".pdf\") and os.path.isfile(file_path):\n",
    "        text = extract_text_from_pdf(file_path)\n",
    "        if not text:\n",
    "            continue\n",
    "        cnpj = extract_cnpj(text)\n",
    "        stats = get_text_stats(text)\n",
    "        stats['File Name'] = filename\n",
    "        stats['CNPJ'] = cnpj\n",
    "        stats['Page Count'] = len(pdfplumber.open(file_path).pages)\n",
    "        stats['File Size (KB)'] = round(Path(file_path).stat().st_size / 1024, 2)\n",
    "        data.append(stats)\n",
    "\n",
    "result_df = pd.DataFrame(data)\n",
    "column_order = ['File Name', 'CNPJ'] + [col for col in result_df.columns if col not in ['File Name', 'CNPJ']]\n",
    "result_df = result_df[column_order]\n",
    "\n",
    "# üîπ Salvar no Excel\n",
    "result_df.to_excel(output_file, index=False, engine=\"openpyxl\")\n",
    "print(f\"‚úÖ Processamento conclu√≠do! Dados salvos em: {output_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EXTRA√á√ÉO - 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Processamento conclu√≠do! Dados salvos em: F:\\BACKUP GERAL 01.06.2020\\UFSC\\2025\\Editais\\Concurso COLLABCOOP\\Output final\\estatutos_resultados_2019.xlsx\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pdfplumber\n",
    "import pytesseract\n",
    "from pdf2image import convert_from_path\n",
    "import pandas as pd\n",
    "import re\n",
    "from math import sqrt\n",
    "from pathlib import Path\n",
    "\n",
    "# üîπ Carregar dicion√°rio de palavras dif√≠ceis para New Dale-Chall Score\n",
    "with open(\"F:\\\\BACKUP GERAL 01.06.2020\\\\UFSC\\\\2025\\\\Editais\\\\Concurso COLLABCOOP\\\\DICION√ÅRIO\\\\dicionario_palavras_dificeis.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    palavras_dificeis = set(f.read().splitlines())\n",
    "\n",
    "# üîπ Fun√ß√£o para calcular √≠ndices de legibilidade\n",
    "def calculate_readability_indices(text):\n",
    "    patterns_to_remove = [\n",
    "        r'^\\s*T√çTULO\\s+[IVXLCDM\\d]+',\n",
    "        r'^\\s*CAP√çTULO\\s+[IVXLCDM\\d]+',\n",
    "        r'^\\s*SE√á√ÉO\\s+[IVXLCDM\\d]+',\n",
    "        r'^\\s*SUBSE√á√ÉO\\s+[IVXLCDM\\d]+',\n",
    "        r'^\\s*Art\\.?\\s*\\d+[¬∫¬∞]?',\n",
    "        r'^\\s*¬ß\\s*\\d+[¬∫¬∞]?',\n",
    "        r'^\\s*Par√°grafo √∫nico\\.',\n",
    "    ]\n",
    "\n",
    "    sentences = re.split(r'(?<=[.!?])\\s+|\\nArt\\.\\s*\\d+', text)\n",
    "    sentences = [\n",
    "        sentence.strip() for sentence in sentences\n",
    "        if sentence.strip() and not any(re.match(pattern, sentence.strip(), re.IGNORECASE) for pattern in patterns_to_remove)\n",
    "    ]\n",
    "    S = max(len(sentences), 1)\n",
    "\n",
    "    words = text.split()\n",
    "    long_words = [w for w in words if len(w) > 6]\n",
    "    complex_words = [w for w in words if len(re.findall(r'[aeiou√°√©√≠√≥√∫√¢√™√Æ√¥√ª√£√µ√†]{2,}', w, re.I)) > 1]\n",
    "    syllables = sum(len(re.findall(r'[aeiou√°√©√≠√≥√∫√¢√™√Æ√¥√ª√£√µ√†]{1,2}', w, re.I)) for w in words)\n",
    "\n",
    "    W = len(words)\n",
    "    LW = len(long_words)\n",
    "    CW = len(complex_words)\n",
    "    C = sum(len(word) for word in words if word.isalpha())\n",
    "\n",
    "    paragraph_count = len(re.findall(r'\\n\\s*\\n', text))\n",
    "    if paragraph_count == 0:\n",
    "        paragraph_count = max(S // 3, 1)\n",
    "\n",
    "    flesch = round(0.39 * (W / S) + 11.8 * (syllables / W) - 15.59, 2) if W > 0 and S > 0 else 0\n",
    "    ari = round(4.71 * (C / W) + 0.5 * (W / S) - 21.43, 2) if W > 0 and S > 0 else 0\n",
    "    lix = round((100 * LW / W) + (W / S), 2) if W > 0 else 0\n",
    "    rix = round(LW / S, 2) if S > 0 else 0\n",
    "    fog = round(0.4 * ((W / S) + (40 * CW / W)), 2) if W > 0 and S > 0 else 0\n",
    "    smog = round(1.043 * sqrt(30 * (CW / S)) + 3.1291, 2) if S > 0 and CW > 0 else 0\n",
    "\n",
    "    difficult_words = sum(1 for w in words if w.lower() in palavras_dificeis)\n",
    "    ndc = round(0.1579 * (difficult_words / W * 100) + 0.0496 * (W / S) + 3.6365, 2) if W > 0 and S > 0 else 0\n",
    "\n",
    "    ttr = round(len(set(words)) / W, 4) if W > 0 else 0\n",
    "    herdan_c = round(sqrt(len(set(words)) / W), 4) if W > 0 else 0\n",
    "\n",
    "    return {\n",
    "        'LIX Index': lix,\n",
    "        'RIX Index': rix,\n",
    "        'ARI Index': ari,\n",
    "        'Flesch-Kincaid': flesch,\n",
    "        'Fog Index': fog,\n",
    "        'SMOG Index': smog,\n",
    "        'New Dale-Chall': ndc,\n",
    "        'Type-Token Ratio (TTR)': ttr,\n",
    "        'Herdan‚Äôs C': herdan_c,\n",
    "        'Paragraph Count': paragraph_count\n",
    "    }\n",
    "\n",
    "# üîπ Fun√ß√£o para contar se√ß√µes, cap√≠tulos e artigos\n",
    "def count_sections(text):\n",
    "    title_count = len(re.findall(r'^\\s*T√çTULO\\s+[IVXLCDM\\d]+\\s*$', text, re.MULTILINE | re.IGNORECASE))\n",
    "    chapter_count = len(re.findall(r'^\\s*CAP√çTULO\\s+[IVXLCDM\\d]+\\s*$', text, re.MULTILINE | re.IGNORECASE))\n",
    "    section_count = len(re.findall(r'^\\s*SE√á√ÉO\\s+[IVXLCDM\\d]+\\s*$', text, re.MULTILINE | re.IGNORECASE))\n",
    "    article_count = len(re.findall(r'^\\s*Art\\.?\\s*\\d+', text, re.MULTILINE | re.IGNORECASE))\n",
    "\n",
    "    return {\n",
    "        'Title Count': title_count,\n",
    "        'Chapter Count': chapter_count,\n",
    "        'Section Count': section_count,\n",
    "        'Article Count': article_count\n",
    "    }\n",
    "\n",
    "# üîπ Fun√ß√£o para extrair CNPJ\n",
    "def extract_cnpj(text):\n",
    "    match = re.search(r'CNPJ\\D*(\\d{2}\\.?\\d{3}\\.?\\d{3}/?\\d{4}-?\\d{2})', text)\n",
    "    return match.group(1).replace(\".\", \"\").replace(\"/\", \"\").replace(\"-\", \"\") if match else None\n",
    "\n",
    "# üîπ Fun√ß√£o para calcular estat√≠sticas do texto\n",
    "def get_text_stats(text):\n",
    "    words = text.split()\n",
    "    sentences = re.split(r'(?<=[.!?])\\s+|\\nArt\\.\\s*\\d+', text)\n",
    "    character_count = sum(len(word) for word in words)\n",
    "    unique_words = len(set(words))\n",
    "    readability_indices = calculate_readability_indices(text)\n",
    "    section_counts = count_sections(text)\n",
    "\n",
    "    return {\n",
    "        'Word Count': len(words),\n",
    "        'Sentence Count': len(sentences),\n",
    "        'Character Count': character_count,\n",
    "        'Unique Words': unique_words,\n",
    "        **readability_indices,\n",
    "        **section_counts\n",
    "    }\n",
    "\n",
    "# üîπ Processar os PDFs\n",
    "year = 2019\n",
    "directory_path = fr\"F:\\BACKUP GERAL 01.06.2020\\UFSC\\2024\\Artigo 2 - TESE\\Estatutos\\{year}\"\n",
    "output_directory = fr\"F:\\BACKUP GERAL 01.06.2020\\UFSC\\2025\\Editais\\Concurso COLLABCOOP\\Output final\"\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "output_file = os.path.join(output_directory, f\"estatutos_resultados_{year}.xlsx\")\n",
    "\n",
    "data = []\n",
    "for filename in os.listdir(directory_path):\n",
    "    file_path = os.path.join(directory_path, filename)\n",
    "    if filename.endswith(\".pdf\") and os.path.isfile(file_path):\n",
    "        text = extract_text_from_pdf(file_path)\n",
    "        if not text:\n",
    "            continue\n",
    "        cnpj = extract_cnpj(text)\n",
    "        stats = get_text_stats(text)\n",
    "        stats['File Name'] = filename\n",
    "        stats['CNPJ'] = cnpj\n",
    "        stats['Page Count'] = len(pdfplumber.open(file_path).pages)\n",
    "        stats['File Size (KB)'] = round(Path(file_path).stat().st_size / 1024, 2)\n",
    "        data.append(stats)\n",
    "\n",
    "result_df = pd.DataFrame(data)\n",
    "column_order = ['File Name', 'CNPJ'] + [col for col in result_df.columns if col not in ['File Name', 'CNPJ']]\n",
    "result_df = result_df[column_order]\n",
    "\n",
    "# üîπ Salvar no Excel\n",
    "result_df.to_excel(output_file, index=False, engine=\"openpyxl\")\n",
    "print(f\"‚úÖ Processamento conclu√≠do! Dados salvos em: {output_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EXTRA√á√ÉO - 2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Processamento conclu√≠do! Dados salvos em: F:\\BACKUP GERAL 01.06.2020\\UFSC\\2025\\Editais\\Concurso COLLABCOOP\\Output final\\estatutos_resultados_2020.xlsx\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pdfplumber\n",
    "import pytesseract\n",
    "from pdf2image import convert_from_path\n",
    "import pandas as pd\n",
    "import re\n",
    "from math import sqrt\n",
    "from pathlib import Path\n",
    "\n",
    "# üîπ Carregar dicion√°rio de palavras dif√≠ceis para New Dale-Chall Score\n",
    "with open(\"F:\\\\BACKUP GERAL 01.06.2020\\\\UFSC\\\\2025\\\\Editais\\\\Concurso COLLABCOOP\\\\DICION√ÅRIO\\\\dicionario_palavras_dificeis.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    palavras_dificeis = set(f.read().splitlines())\n",
    "\n",
    "# üîπ Fun√ß√£o para calcular √≠ndices de legibilidade\n",
    "def calculate_readability_indices(text):\n",
    "    patterns_to_remove = [\n",
    "        r'^\\s*T√çTULO\\s+[IVXLCDM\\d]+',\n",
    "        r'^\\s*CAP√çTULO\\s+[IVXLCDM\\d]+',\n",
    "        r'^\\s*SE√á√ÉO\\s+[IVXLCDM\\d]+',\n",
    "        r'^\\s*SUBSE√á√ÉO\\s+[IVXLCDM\\d]+',\n",
    "        r'^\\s*Art\\.?\\s*\\d+[¬∫¬∞]?',\n",
    "        r'^\\s*¬ß\\s*\\d+[¬∫¬∞]?',\n",
    "        r'^\\s*Par√°grafo √∫nico\\.',\n",
    "    ]\n",
    "\n",
    "    sentences = re.split(r'(?<=[.!?])\\s+|\\nArt\\.\\s*\\d+', text)\n",
    "    sentences = [\n",
    "        sentence.strip() for sentence in sentences\n",
    "        if sentence.strip() and not any(re.match(pattern, sentence.strip(), re.IGNORECASE) for pattern in patterns_to_remove)\n",
    "    ]\n",
    "    S = max(len(sentences), 1)\n",
    "\n",
    "    words = text.split()\n",
    "    long_words = [w for w in words if len(w) > 6]\n",
    "    complex_words = [w for w in words if len(re.findall(r'[aeiou√°√©√≠√≥√∫√¢√™√Æ√¥√ª√£√µ√†]{2,}', w, re.I)) > 1]\n",
    "    syllables = sum(len(re.findall(r'[aeiou√°√©√≠√≥√∫√¢√™√Æ√¥√ª√£√µ√†]{1,2}', w, re.I)) for w in words)\n",
    "\n",
    "    W = len(words)\n",
    "    LW = len(long_words)\n",
    "    CW = len(complex_words)\n",
    "    C = sum(len(word) for word in words if word.isalpha())\n",
    "\n",
    "    paragraph_count = len(re.findall(r'\\n\\s*\\n', text))\n",
    "    if paragraph_count == 0:\n",
    "        paragraph_count = max(S // 3, 1)\n",
    "\n",
    "    flesch = round(0.39 * (W / S) + 11.8 * (syllables / W) - 15.59, 2) if W > 0 and S > 0 else 0\n",
    "    ari = round(4.71 * (C / W) + 0.5 * (W / S) - 21.43, 2) if W > 0 and S > 0 else 0\n",
    "    lix = round((100 * LW / W) + (W / S), 2) if W > 0 else 0\n",
    "    rix = round(LW / S, 2) if S > 0 else 0\n",
    "    fog = round(0.4 * ((W / S) + (40 * CW / W)), 2) if W > 0 and S > 0 else 0\n",
    "    smog = round(1.043 * sqrt(30 * (CW / S)) + 3.1291, 2) if S > 0 and CW > 0 else 0\n",
    "\n",
    "    difficult_words = sum(1 for w in words if w.lower() in palavras_dificeis)\n",
    "    ndc = round(0.1579 * (difficult_words / W * 100) + 0.0496 * (W / S) + 3.6365, 2) if W > 0 and S > 0 else 0\n",
    "\n",
    "    ttr = round(len(set(words)) / W, 4) if W > 0 else 0\n",
    "    herdan_c = round(sqrt(len(set(words)) / W), 4) if W > 0 else 0\n",
    "\n",
    "    return {\n",
    "        'LIX Index': lix,\n",
    "        'RIX Index': rix,\n",
    "        'ARI Index': ari,\n",
    "        'Flesch-Kincaid': flesch,\n",
    "        'Fog Index': fog,\n",
    "        'SMOG Index': smog,\n",
    "        'New Dale-Chall': ndc,\n",
    "        'Type-Token Ratio (TTR)': ttr,\n",
    "        'Herdan‚Äôs C': herdan_c,\n",
    "        'Paragraph Count': paragraph_count\n",
    "    }\n",
    "\n",
    "# üîπ Fun√ß√£o para contar se√ß√µes, cap√≠tulos e artigos\n",
    "def count_sections(text):\n",
    "    title_count = len(re.findall(r'^\\s*T√çTULO\\s+[IVXLCDM\\d]+\\s*$', text, re.MULTILINE | re.IGNORECASE))\n",
    "    chapter_count = len(re.findall(r'^\\s*CAP√çTULO\\s+[IVXLCDM\\d]+\\s*$', text, re.MULTILINE | re.IGNORECASE))\n",
    "    section_count = len(re.findall(r'^\\s*SE√á√ÉO\\s+[IVXLCDM\\d]+\\s*$', text, re.MULTILINE | re.IGNORECASE))\n",
    "    article_count = len(re.findall(r'^\\s*Art\\.?\\s*\\d+', text, re.MULTILINE | re.IGNORECASE))\n",
    "\n",
    "    return {\n",
    "        'Title Count': title_count,\n",
    "        'Chapter Count': chapter_count,\n",
    "        'Section Count': section_count,\n",
    "        'Article Count': article_count\n",
    "    }\n",
    "\n",
    "# üîπ Fun√ß√£o para extrair CNPJ\n",
    "def extract_cnpj(text):\n",
    "    match = re.search(r'CNPJ\\D*(\\d{2}\\.?\\d{3}\\.?\\d{3}/?\\d{4}-?\\d{2})', text)\n",
    "    return match.group(1).replace(\".\", \"\").replace(\"/\", \"\").replace(\"-\", \"\") if match else None\n",
    "\n",
    "# üîπ Fun√ß√£o para calcular estat√≠sticas do texto\n",
    "def get_text_stats(text):\n",
    "    words = text.split()\n",
    "    sentences = re.split(r'(?<=[.!?])\\s+|\\nArt\\.\\s*\\d+', text)\n",
    "    character_count = sum(len(word) for word in words)\n",
    "    unique_words = len(set(words))\n",
    "    readability_indices = calculate_readability_indices(text)\n",
    "    section_counts = count_sections(text)\n",
    "\n",
    "    return {\n",
    "        'Word Count': len(words),\n",
    "        'Sentence Count': len(sentences),\n",
    "        'Character Count': character_count,\n",
    "        'Unique Words': unique_words,\n",
    "        **readability_indices,\n",
    "        **section_counts\n",
    "    }\n",
    "\n",
    "# üîπ Processar os PDFs\n",
    "year = 2020\n",
    "directory_path = fr\"F:\\BACKUP GERAL 01.06.2020\\UFSC\\2024\\Artigo 2 - TESE\\Estatutos\\{year}\"\n",
    "output_directory = fr\"F:\\BACKUP GERAL 01.06.2020\\UFSC\\2025\\Editais\\Concurso COLLABCOOP\\Output final\"\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "output_file = os.path.join(output_directory, f\"estatutos_resultados_{year}.xlsx\")\n",
    "\n",
    "data = []\n",
    "for filename in os.listdir(directory_path):\n",
    "    file_path = os.path.join(directory_path, filename)\n",
    "    if filename.endswith(\".pdf\") and os.path.isfile(file_path):\n",
    "        text = extract_text_from_pdf(file_path)\n",
    "        if not text:\n",
    "            continue\n",
    "        cnpj = extract_cnpj(text)\n",
    "        stats = get_text_stats(text)\n",
    "        stats['File Name'] = filename\n",
    "        stats['CNPJ'] = cnpj\n",
    "        stats['Page Count'] = len(pdfplumber.open(file_path).pages)\n",
    "        stats['File Size (KB)'] = round(Path(file_path).stat().st_size / 1024, 2)\n",
    "        data.append(stats)\n",
    "\n",
    "result_df = pd.DataFrame(data)\n",
    "column_order = ['File Name', 'CNPJ'] + [col for col in result_df.columns if col not in ['File Name', 'CNPJ']]\n",
    "result_df = result_df[column_order]\n",
    "\n",
    "# üîπ Salvar no Excel\n",
    "result_df.to_excel(output_file, index=False, engine=\"openpyxl\")\n",
    "print(f\"‚úÖ Processamento conclu√≠do! Dados salvos em: {output_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EXTRA√á√ÉO - 2021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Processamento conclu√≠do! Dados salvos em: F:\\BACKUP GERAL 01.06.2020\\UFSC\\2025\\Editais\\Concurso COLLABCOOP\\Output final\\estatutos_resultados_2021.xlsx\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pdfplumber\n",
    "import pytesseract\n",
    "from pdf2image import convert_from_path\n",
    "import pandas as pd\n",
    "import re\n",
    "from math import sqrt\n",
    "from pathlib import Path\n",
    "\n",
    "# üîπ Carregar dicion√°rio de palavras dif√≠ceis para New Dale-Chall Score\n",
    "with open(\"F:\\\\BACKUP GERAL 01.06.2020\\\\UFSC\\\\2025\\\\Editais\\\\Concurso COLLABCOOP\\\\DICION√ÅRIO\\\\dicionario_palavras_dificeis.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    palavras_dificeis = set(f.read().splitlines())\n",
    "\n",
    "# üîπ Fun√ß√£o para calcular √≠ndices de legibilidade\n",
    "def calculate_readability_indices(text):\n",
    "    patterns_to_remove = [\n",
    "        r'^\\s*T√çTULO\\s+[IVXLCDM\\d]+',\n",
    "        r'^\\s*CAP√çTULO\\s+[IVXLCDM\\d]+',\n",
    "        r'^\\s*SE√á√ÉO\\s+[IVXLCDM\\d]+',\n",
    "        r'^\\s*SUBSE√á√ÉO\\s+[IVXLCDM\\d]+',\n",
    "        r'^\\s*Art\\.?\\s*\\d+[¬∫¬∞]?',\n",
    "        r'^\\s*¬ß\\s*\\d+[¬∫¬∞]?',\n",
    "        r'^\\s*Par√°grafo √∫nico\\.',\n",
    "    ]\n",
    "\n",
    "    sentences = re.split(r'(?<=[.!?])\\s+|\\nArt\\.\\s*\\d+', text)\n",
    "    sentences = [\n",
    "        sentence.strip() for sentence in sentences\n",
    "        if sentence.strip() and not any(re.match(pattern, sentence.strip(), re.IGNORECASE) for pattern in patterns_to_remove)\n",
    "    ]\n",
    "    S = max(len(sentences), 1)\n",
    "\n",
    "    words = text.split()\n",
    "    long_words = [w for w in words if len(w) > 6]\n",
    "    complex_words = [w for w in words if len(re.findall(r'[aeiou√°√©√≠√≥√∫√¢√™√Æ√¥√ª√£√µ√†]{2,}', w, re.I)) > 1]\n",
    "    syllables = sum(len(re.findall(r'[aeiou√°√©√≠√≥√∫√¢√™√Æ√¥√ª√£√µ√†]{1,2}', w, re.I)) for w in words)\n",
    "\n",
    "    W = len(words)\n",
    "    LW = len(long_words)\n",
    "    CW = len(complex_words)\n",
    "    C = sum(len(word) for word in words if word.isalpha())\n",
    "\n",
    "    paragraph_count = len(re.findall(r'\\n\\s*\\n', text))\n",
    "    if paragraph_count == 0:\n",
    "        paragraph_count = max(S // 3, 1)\n",
    "\n",
    "    flesch = round(0.39 * (W / S) + 11.8 * (syllables / W) - 15.59, 2) if W > 0 and S > 0 else 0\n",
    "    ari = round(4.71 * (C / W) + 0.5 * (W / S) - 21.43, 2) if W > 0 and S > 0 else 0\n",
    "    lix = round((100 * LW / W) + (W / S), 2) if W > 0 else 0\n",
    "    rix = round(LW / S, 2) if S > 0 else 0\n",
    "    fog = round(0.4 * ((W / S) + (40 * CW / W)), 2) if W > 0 and S > 0 else 0\n",
    "    smog = round(1.043 * sqrt(30 * (CW / S)) + 3.1291, 2) if S > 0 and CW > 0 else 0\n",
    "\n",
    "    difficult_words = sum(1 for w in words if w.lower() in palavras_dificeis)\n",
    "    ndc = round(0.1579 * (difficult_words / W * 100) + 0.0496 * (W / S) + 3.6365, 2) if W > 0 and S > 0 else 0\n",
    "\n",
    "    ttr = round(len(set(words)) / W, 4) if W > 0 else 0\n",
    "    herdan_c = round(sqrt(len(set(words)) / W), 4) if W > 0 else 0\n",
    "\n",
    "    return {\n",
    "        'LIX Index': lix,\n",
    "        'RIX Index': rix,\n",
    "        'ARI Index': ari,\n",
    "        'Flesch-Kincaid': flesch,\n",
    "        'Fog Index': fog,\n",
    "        'SMOG Index': smog,\n",
    "        'New Dale-Chall': ndc,\n",
    "        'Type-Token Ratio (TTR)': ttr,\n",
    "        'Herdan‚Äôs C': herdan_c,\n",
    "        'Paragraph Count': paragraph_count\n",
    "    }\n",
    "\n",
    "# üîπ Fun√ß√£o para contar se√ß√µes, cap√≠tulos e artigos\n",
    "def count_sections(text):\n",
    "    title_count = len(re.findall(r'^\\s*T√çTULO\\s+[IVXLCDM\\d]+\\s*$', text, re.MULTILINE | re.IGNORECASE))\n",
    "    chapter_count = len(re.findall(r'^\\s*CAP√çTULO\\s+[IVXLCDM\\d]+\\s*$', text, re.MULTILINE | re.IGNORECASE))\n",
    "    section_count = len(re.findall(r'^\\s*SE√á√ÉO\\s+[IVXLCDM\\d]+\\s*$', text, re.MULTILINE | re.IGNORECASE))\n",
    "    article_count = len(re.findall(r'^\\s*Art\\.?\\s*\\d+', text, re.MULTILINE | re.IGNORECASE))\n",
    "\n",
    "    return {\n",
    "        'Title Count': title_count,\n",
    "        'Chapter Count': chapter_count,\n",
    "        'Section Count': section_count,\n",
    "        'Article Count': article_count\n",
    "    }\n",
    "\n",
    "# üîπ Fun√ß√£o para extrair CNPJ\n",
    "def extract_cnpj(text):\n",
    "    match = re.search(r'CNPJ\\D*(\\d{2}\\.?\\d{3}\\.?\\d{3}/?\\d{4}-?\\d{2})', text)\n",
    "    return match.group(1).replace(\".\", \"\").replace(\"/\", \"\").replace(\"-\", \"\") if match else None\n",
    "\n",
    "# üîπ Fun√ß√£o para calcular estat√≠sticas do texto\n",
    "def get_text_stats(text):\n",
    "    words = text.split()\n",
    "    sentences = re.split(r'(?<=[.!?])\\s+|\\nArt\\.\\s*\\d+', text)\n",
    "    character_count = sum(len(word) for word in words)\n",
    "    unique_words = len(set(words))\n",
    "    readability_indices = calculate_readability_indices(text)\n",
    "    section_counts = count_sections(text)\n",
    "\n",
    "    return {\n",
    "        'Word Count': len(words),\n",
    "        'Sentence Count': len(sentences),\n",
    "        'Character Count': character_count,\n",
    "        'Unique Words': unique_words,\n",
    "        **readability_indices,\n",
    "        **section_counts\n",
    "    }\n",
    "\n",
    "# üîπ Processar os PDFs\n",
    "year = 2021\n",
    "directory_path = fr\"F:\\BACKUP GERAL 01.06.2020\\UFSC\\2024\\Artigo 2 - TESE\\Estatutos\\{year}\"\n",
    "output_directory = fr\"F:\\BACKUP GERAL 01.06.2020\\UFSC\\2025\\Editais\\Concurso COLLABCOOP\\Output final\"\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "output_file = os.path.join(output_directory, f\"estatutos_resultados_{year}.xlsx\")\n",
    "\n",
    "data = []\n",
    "for filename in os.listdir(directory_path):\n",
    "    file_path = os.path.join(directory_path, filename)\n",
    "    if filename.endswith(\".pdf\") and os.path.isfile(file_path):\n",
    "        text = extract_text_from_pdf(file_path)\n",
    "        if not text:\n",
    "            continue\n",
    "        cnpj = extract_cnpj(text)\n",
    "        stats = get_text_stats(text)\n",
    "        stats['File Name'] = filename\n",
    "        stats['CNPJ'] = cnpj\n",
    "        stats['Page Count'] = len(pdfplumber.open(file_path).pages)\n",
    "        stats['File Size (KB)'] = round(Path(file_path).stat().st_size / 1024, 2)\n",
    "        data.append(stats)\n",
    "\n",
    "result_df = pd.DataFrame(data)\n",
    "column_order = ['File Name', 'CNPJ'] + [col for col in result_df.columns if col not in ['File Name', 'CNPJ']]\n",
    "result_df = result_df[column_order]\n",
    "\n",
    "# üîπ Salvar no Excel\n",
    "result_df.to_excel(output_file, index=False, engine=\"openpyxl\")\n",
    "print(f\"‚úÖ Processamento conclu√≠do! Dados salvos em: {output_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EXTRA√á√ÉO - 2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Processamento conclu√≠do! Dados salvos em: F:\\BACKUP GERAL 01.06.2020\\UFSC\\2025\\Editais\\Concurso COLLABCOOP\\Output final\\estatutos_resultados_2022.xlsx\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pdfplumber\n",
    "import pytesseract\n",
    "from pdf2image import convert_from_path\n",
    "import pandas as pd\n",
    "import re\n",
    "from math import sqrt\n",
    "from pathlib import Path\n",
    "\n",
    "# üîπ Carregar dicion√°rio de palavras dif√≠ceis para New Dale-Chall Score\n",
    "with open(\"F:\\\\BACKUP GERAL 01.06.2020\\\\UFSC\\\\2025\\\\Editais\\\\Concurso COLLABCOOP\\\\DICION√ÅRIO\\\\dicionario_palavras_dificeis.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    palavras_dificeis = set(f.read().splitlines())\n",
    "\n",
    "# üîπ Fun√ß√£o para calcular √≠ndices de legibilidade\n",
    "def calculate_readability_indices(text):\n",
    "    patterns_to_remove = [\n",
    "        r'^\\s*T√çTULO\\s+[IVXLCDM\\d]+',\n",
    "        r'^\\s*CAP√çTULO\\s+[IVXLCDM\\d]+',\n",
    "        r'^\\s*SE√á√ÉO\\s+[IVXLCDM\\d]+',\n",
    "        r'^\\s*SUBSE√á√ÉO\\s+[IVXLCDM\\d]+',\n",
    "        r'^\\s*Art\\.?\\s*\\d+[¬∫¬∞]?',\n",
    "        r'^\\s*¬ß\\s*\\d+[¬∫¬∞]?',\n",
    "        r'^\\s*Par√°grafo √∫nico\\.',\n",
    "    ]\n",
    "\n",
    "    sentences = re.split(r'(?<=[.!?])\\s+|\\nArt\\.\\s*\\d+', text)\n",
    "    sentences = [\n",
    "        sentence.strip() for sentence in sentences\n",
    "        if sentence.strip() and not any(re.match(pattern, sentence.strip(), re.IGNORECASE) for pattern in patterns_to_remove)\n",
    "    ]\n",
    "    S = max(len(sentences), 1)\n",
    "\n",
    "    words = text.split()\n",
    "    long_words = [w for w in words if len(w) > 6]\n",
    "    complex_words = [w for w in words if len(re.findall(r'[aeiou√°√©√≠√≥√∫√¢√™√Æ√¥√ª√£√µ√†]{2,}', w, re.I)) > 1]\n",
    "    syllables = sum(len(re.findall(r'[aeiou√°√©√≠√≥√∫√¢√™√Æ√¥√ª√£√µ√†]{1,2}', w, re.I)) for w in words)\n",
    "\n",
    "    W = len(words)\n",
    "    LW = len(long_words)\n",
    "    CW = len(complex_words)\n",
    "    C = sum(len(word) for word in words if word.isalpha())\n",
    "\n",
    "    paragraph_count = len(re.findall(r'\\n\\s*\\n', text))\n",
    "    if paragraph_count == 0:\n",
    "        paragraph_count = max(S // 3, 1)\n",
    "\n",
    "    flesch = round(0.39 * (W / S) + 11.8 * (syllables / W) - 15.59, 2) if W > 0 and S > 0 else 0\n",
    "    ari = round(4.71 * (C / W) + 0.5 * (W / S) - 21.43, 2) if W > 0 and S > 0 else 0\n",
    "    lix = round((100 * LW / W) + (W / S), 2) if W > 0 else 0\n",
    "    rix = round(LW / S, 2) if S > 0 else 0\n",
    "    fog = round(0.4 * ((W / S) + (40 * CW / W)), 2) if W > 0 and S > 0 else 0\n",
    "    smog = round(1.043 * sqrt(30 * (CW / S)) + 3.1291, 2) if S > 0 and CW > 0 else 0\n",
    "\n",
    "    difficult_words = sum(1 for w in words if w.lower() in palavras_dificeis)\n",
    "    ndc = round(0.1579 * (difficult_words / W * 100) + 0.0496 * (W / S) + 3.6365, 2) if W > 0 and S > 0 else 0\n",
    "\n",
    "    ttr = round(len(set(words)) / W, 4) if W > 0 else 0\n",
    "    herdan_c = round(sqrt(len(set(words)) / W), 4) if W > 0 else 0\n",
    "\n",
    "    return {\n",
    "        'LIX Index': lix,\n",
    "        'RIX Index': rix,\n",
    "        'ARI Index': ari,\n",
    "        'Flesch-Kincaid': flesch,\n",
    "        'Fog Index': fog,\n",
    "        'SMOG Index': smog,\n",
    "        'New Dale-Chall': ndc,\n",
    "        'Type-Token Ratio (TTR)': ttr,\n",
    "        'Herdan‚Äôs C': herdan_c,\n",
    "        'Paragraph Count': paragraph_count\n",
    "    }\n",
    "\n",
    "# üîπ Fun√ß√£o para contar se√ß√µes, cap√≠tulos e artigos\n",
    "def count_sections(text):\n",
    "    title_count = len(re.findall(r'^\\s*T√çTULO\\s+[IVXLCDM\\d]+\\s*$', text, re.MULTILINE | re.IGNORECASE))\n",
    "    chapter_count = len(re.findall(r'^\\s*CAP√çTULO\\s+[IVXLCDM\\d]+\\s*$', text, re.MULTILINE | re.IGNORECASE))\n",
    "    section_count = len(re.findall(r'^\\s*SE√á√ÉO\\s+[IVXLCDM\\d]+\\s*$', text, re.MULTILINE | re.IGNORECASE))\n",
    "    article_count = len(re.findall(r'^\\s*Art\\.?\\s*\\d+', text, re.MULTILINE | re.IGNORECASE))\n",
    "\n",
    "    return {\n",
    "        'Title Count': title_count,\n",
    "        'Chapter Count': chapter_count,\n",
    "        'Section Count': section_count,\n",
    "        'Article Count': article_count\n",
    "    }\n",
    "\n",
    "# üîπ Fun√ß√£o para extrair CNPJ\n",
    "def extract_cnpj(text):\n",
    "    match = re.search(r'CNPJ\\D*(\\d{2}\\.?\\d{3}\\.?\\d{3}/?\\d{4}-?\\d{2})', text)\n",
    "    return match.group(1).replace(\".\", \"\").replace(\"/\", \"\").replace(\"-\", \"\") if match else None\n",
    "\n",
    "# üîπ Fun√ß√£o para calcular estat√≠sticas do texto\n",
    "def get_text_stats(text):\n",
    "    words = text.split()\n",
    "    sentences = re.split(r'(?<=[.!?])\\s+|\\nArt\\.\\s*\\d+', text)\n",
    "    character_count = sum(len(word) for word in words)\n",
    "    unique_words = len(set(words))\n",
    "    readability_indices = calculate_readability_indices(text)\n",
    "    section_counts = count_sections(text)\n",
    "\n",
    "    return {\n",
    "        'Word Count': len(words),\n",
    "        'Sentence Count': len(sentences),\n",
    "        'Character Count': character_count,\n",
    "        'Unique Words': unique_words,\n",
    "        **readability_indices,\n",
    "        **section_counts\n",
    "    }\n",
    "\n",
    "# üîπ Processar os PDFs\n",
    "year = 2022\n",
    "directory_path = fr\"F:\\BACKUP GERAL 01.06.2020\\UFSC\\2024\\Artigo 2 - TESE\\Estatutos\\{year}\"\n",
    "output_directory = fr\"F:\\BACKUP GERAL 01.06.2020\\UFSC\\2025\\Editais\\Concurso COLLABCOOP\\Output final\"\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "output_file = os.path.join(output_directory, f\"estatutos_resultados_{year}.xlsx\")\n",
    "\n",
    "data = []\n",
    "for filename in os.listdir(directory_path):\n",
    "    file_path = os.path.join(directory_path, filename)\n",
    "    if filename.endswith(\".pdf\") and os.path.isfile(file_path):\n",
    "        text = extract_text_from_pdf(file_path)\n",
    "        if not text:\n",
    "            continue\n",
    "        cnpj = extract_cnpj(text)\n",
    "        stats = get_text_stats(text)\n",
    "        stats['File Name'] = filename\n",
    "        stats['CNPJ'] = cnpj\n",
    "        stats['Page Count'] = len(pdfplumber.open(file_path).pages)\n",
    "        stats['File Size (KB)'] = round(Path(file_path).stat().st_size / 1024, 2)\n",
    "        data.append(stats)\n",
    "\n",
    "result_df = pd.DataFrame(data)\n",
    "column_order = ['File Name', 'CNPJ'] + [col for col in result_df.columns if col not in ['File Name', 'CNPJ']]\n",
    "result_df = result_df[column_order]\n",
    "\n",
    "# üîπ Salvar no Excel\n",
    "result_df.to_excel(output_file, index=False, engine=\"openpyxl\")\n",
    "print(f\"‚úÖ Processamento conclu√≠do! Dados salvos em: {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (4079094119.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\Arthur\\AppData\\Local\\Temp\\ipykernel_18532\\4079094119.py\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    jupyter nbconvert --to docx \"Extract final.ipynb\"\u001b[0m\n\u001b[1;37m            ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "jupyter nbconvert --to docx \"Extract final.ipynb\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
